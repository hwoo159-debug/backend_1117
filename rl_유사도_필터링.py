# -*- coding: utf-8 -*-
"""RL/유사도/필터링

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LrE34OqgsgJ2wDi2GbdBttbSsVROaz2e
"""

# rag_rl_integrated_system.py
# 강화학습 + 벡터 검색 + 필터링 통합 시스템
# 당신의 기존 코드 확장 버전

import os
import gc
import ctypes
import platform
import numpy as np
import json
import re
from typing import Dict, List, Tuple, Optional
from urllib.parse import urlparse
from dataclasses import dataclass, asdict
from datetime import datetime
import warnings
warnings.filterwarnings("ignore")
############################################################
# Memory Cleanup
############################################################

def aggressive_memory_cleanup():
    gc.collect()
    if platform.system() == "Linux":
        try:
            ctypes.CDLL("libc.so.6").malloc_trim(0)
        except:
            pass

############################################################
# Groq LLM
############################################################

from openai import OpenAI

client = OpenAI(
    api_key=os.getenv("GROQ_API_KEY"),
    base_url=os.getenv("GROQ_API_URL", "https://api.groq.com/openai/v1")
)

GROQ_MODEL = os.getenv("GROQ_MODEL", "llama3-70b-8192")

def call_llm_api(prompt: str, temperature: float = 0.2) -> str:
    try:
        response = client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=512,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[LLM ERROR] {e}")
        return "LLM 생성 오류가 발생했습니다."

############################################################
# 간단 로컬 임베딩
############################################################

LOCAL_EMBED_DIM = int(os.getenv("LOCAL_EMBED_DIM", "512"))
_token_pattern = re.compile(r"[가-힣A-Za-z0-9]+")

def simple_tokenize(text: str) -> List[str]:
    text = "" if text is None else str(text)
    return _token_pattern.findall(text.lower())

def simple_embed(texts: List[str], dim: int = LOCAL_EMBED_DIM) -> np.ndarray:
    n = len(texts)
    vecs = np.zeros((n, dim), dtype="float32")
    for i, t in enumerate(texts):
        tokens = simple_tokenize(t)
        for tok in tokens:
            h = hash(tok) % dim
            vecs[i, h] += 1.0
    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-8
    vecs = vecs / norms
    return vecs

############################################################
# REWARD FUNCTION (새로 추가)
############################################################

@dataclass
class RewardConfig:
    structure_weight: float = 0.3
    information_weight: float = 0.3
    clarity_weight: float = 0.2
    relevance_weight: float = 0.2
    diversity_weight: float = 0.00

class RewardFunction:
    def __init__(self, config: RewardConfig = None):
        self.config = config or RewardConfig()

    def __call__(self, question: str, answer: str,
                 sources: List[str] = None) -> Tuple[float, Dict]:
        sources = sources or []
        breakdown = {}
        total_reward = 0.0

        structure_score = self._eval_structure(answer)
        breakdown['structure'] = structure_score
        total_reward += structure_score * self.config.structure_weight

        information_score = self._eval_information(answer, question)
        breakdown['information'] = information_score
        total_reward += information_score * self.config.information_weight

        clarity_score = self._eval_clarity(answer)
        breakdown['clarity'] = clarity_score
        total_reward += clarity_score * self.config.clarity_weight

        relevance_score = self._eval_relevance(question, answer)
        breakdown['relevance'] = relevance_score
        total_reward += relevance_score * self.config.relevance_weight

        #diversity_score = self._eval_diversity(answer)
        #breakdown['diversity'] = diversity_score
        #total_reward += diversity_score * self.config.diversity_weight

        final_reward = min(total_reward, 1.0)
        breakdown['total'] = final_reward

        return final_reward, breakdown

    def _eval_structure(self, answer: str) -> float:
        score = 0.0
        if re.search(r"제\d+조", answer):
            score += 0.5
        if re.search(r"[0-9]+\)", answer):
            score += 0.3
        if len(answer.split("\n")) >= 2:
            score += 0.2
        return min(score, 1.0)

    def _eval_information(self, answer: str, question: str) -> float:
        score = 0.0
        if re.search(r"\d+년|\d+개월|\d+일", answer):
            score += 0.3
        if re.search(r"\d+원|\d+\%", answer):
            score += 0.3
        if 50 < len(answer) < 2000:
            score += 0.4
        return min(score, 1.0)

    def _eval_clarity(self, answer: str) -> float:
        score = 1.0
        uncertain_phrases = [
            ("아마도", 0.2), ("것 같다", 0.2), ("추정된다", 0.15),
            ("가능할 것 같다", 0.2), ("대체로", 0.1),
        ]
        for phrase, penalty in uncertain_phrases:
            if phrase in answer:
                score -= penalty
        return max(score, 0.0)

    def _eval_relevance(self, question: str, answer: str) -> float:
        q_words = set(simple_tokenize(question))
        a_words = set(simple_tokenize(answer))
        if not q_words:
            return 0.5
        overlap = len(q_words & a_words) / len(q_words)
        return min(overlap, 1.0)

#    def _eval_diversity(self, answer: str) -> float:
#        words = simple_tokenize(answer)
#        if len(words) == 0:
#            return 0.0
#        unique_ratio = len(set(words)) / len(words)
#        if 0.4 <= unique_ratio <= 0.9:
#            return 1.0
#        elif 0.3 <= unique_ratio < 0.4 or 0.9 < unique_ratio <= 1.0:
#            return 0.5
#        else:
#            return 0.0

############################################################
# RESPONSE FILTER (새로 추가)
############################################################

class ResponseFilter:
    def __init__(self):
        self.filters: Dict[str, callable] = {}

    def add_filter(self, name: str, condition: callable):
        self.filters[name] = condition

    def apply_filters(self, answer: str, context: Dict) -> Tuple[bool, List[str]]:
        failed = []
        for name, condition in self.filters.items():
            try:
                if not condition(answer, context):
                    failed.append(name)
            except Exception as e:
                print(f"[FILTER ERROR] {name}: {e}")
                failed.append(name)
        return len(failed) == 0, failed

    @staticmethod
    def create_default_filters() -> "ResponseFilter":
        f = ResponseFilter()
        f.add_filter("length", lambda a, c: 20 < len(a) < 3000)
        f.add_filter("korean", lambda a, c: any("\uac00" <= ch <= "\ud7a3" for ch in a))

        def no_forbidden(answer, context):
            forbidden = ["모릅니다", "알 수 없습니다", "기재되지 않았습니다"]
            return not any(fw in answer for fw in forbidden)
        f.add_filter("no_forbidden", no_forbidden)

        f.add_filter("confidence", lambda a, c: c.get("reward", 0) >= 0.5)
        f.add_filter("sources", lambda a, c: len(c.get("sources", [])) >= 1)

        return f

############################################################
# PPO TRAINER (새로 추가)
############################################################

@dataclass
class RLExperience:
    question: str
    answer: str
    reward: float
    breakdown: Dict
    timestamp: str
    sources: List[str]

class PPOTrainer:
    def __init__(self, reward_fn: RewardFunction,
                 response_filter: ResponseFilter = None):
        self.reward_fn = reward_fn
        self.response_filter = response_filter or ResponseFilter.create_default_filters()
        self.experiences: List[RLExperience] = []
        self.training_stats = {
            "avg_reward": 0.0,
            "max_reward": 0.0,
            "min_reward": 1.0,
            "total_samples": 0,
            "pass_rate": 0.0,
        }
        self.policy_version = 1

    def evaluate_response(self, question: str, answer: str,
                         sources: List[str] = None) -> Dict:
        sources = sources or []
        reward, breakdown = self.reward_fn(question, answer, sources)
        context = {
            "reward": reward,
            "sources": sources,
            "breakdown": breakdown
        }
        passed, failed_filters = self.response_filter.apply_filters(answer, context)
        final_reward = reward if passed else max(reward - 0.3, 0.0)

        return {
            "reward": final_reward,
            "breakdown": breakdown,
            "passed": passed,
            "failed_filters": failed_filters,
            "sources": sources
        }

    def record_experience(self, question: str, answer: str,
                         sources: List[str], reward: float, breakdown: Dict):
        exp = RLExperience(
            question=question,
            answer=answer,
            reward=reward,
            breakdown=breakdown,
            timestamp=datetime.now().isoformat(),
            sources=sources
        )
        self.experiences.append(exp)

    def update_policy(self, learning_rate: float = 0.01):
        if len(self.experiences) < 2:
            return

        rewards = np.array([e.reward for e in self.experiences])
        self.training_stats["avg_reward"] = float(np.mean(rewards))
        self.training_stats["max_reward"] = float(np.max(rewards))
        self.training_stats["min_reward"] = float(np.min(rewards))
        self.training_stats["total_samples"] = len(self.experiences)

        pass_count = sum(1 for e in self.experiences if e.reward >= 0.5)
        self.training_stats["pass_rate"] = pass_count / len(self.experiences)

        self.policy_version += 1

    def get_training_summary(self) -> Dict:
        return {
            "policy_version": self.policy_version,
            "statistics": self.training_stats,
            "total_experiences": len(self.experiences),
            "recent_experiences": [asdict(e) for e in self.experiences[-5:]]
        }

############################################################
# DATABASE CONNECTOR (기존 코드 유지)
############################################################

class DatabaseConnector:
    def __init__(self):
        raw_url = os.getenv("DATABASE_URL")
        if not raw_url:
            raise RuntimeError("환경변수 DATABASE_URL 이 없습니다.")
        if raw_url.startswith("postgres://"):
            raw_url = raw_url.replace("postgres://", "postgresql://", 1)
        parsed = urlparse(raw_url)
        self.config = {
            "host": parsed.hostname,
            "port": parsed.port,
            "database": parsed.path.lstrip("/"),
            "user": parsed.username,
            "password": parsed.password,
        }
        self.connection = None
        print(f"[DB] config = {self.config}")

    def connect(self):
        try:
            import psycopg2
            self.connection = psycopg2.connect(**self.config)
            print("[DB] 연결 성공")
            return True
        except Exception as e:
            print(f"[DB ERROR] {e}")
            return False

    def disconnect(self):
        if self.connection:
            self.connection.close()
            self.connection = None

    def get_policy_chunks(self, limit: int = 20000):
        try:
            from psycopg2.extras import RealDictCursor
            cur = self.connection.cursor(cursor_factory=RealDictCursor)
            cur.execute("""
                SELECT
                    id          AS chunk_id,
                    product_id  AS product_id,
                    COALESCE(search_text, original_text) AS chunk_text,
                    embedding   AS embedding
                FROM insurance_clauses
                WHERE COALESCE(search_text, original_text) IS NOT NULL
                LIMIT %s
            """, (limit,))
            rows = cur.fetchall()
            cur.close()
            print(f"[DB] 보험약관 로드 {len(rows)}개")
            return rows
        except Exception as e:
            print(f"[DB 조회 오류] {e}")
            return []

############################################################
# VECTOR STORE (기존 코드 유지)
############################################################

class VectorStore:
    def __init__(self):
        self.documents: List[Dict] = []
        self.embeddings: Optional[np.ndarray] = None

    def load(self, docs: List[Dict], embeddings: np.ndarray):
        self.documents = docs
        self.embeddings = embeddings
        print(f"[VS] 벡터스토어 구축: {len(docs)}개, dim={embeddings.shape[1]}")

    def search(self, query_embedding: np.ndarray, k: int = 5):
        if self.embeddings is None or len(self.embeddings) == 0:
            print("[VS] 임베딩 없음")
            return []
        q = query_embedding[0]
        d_store = self.embeddings.shape[1]
        d_q = q.shape[0]
        if d_q < d_store:
            q = np.pad(q, (0, d_store - d_q))
        elif d_q > d_store:
            q = q[:d_store]
        sims: List[Tuple[int, float]] = []
        q_norm = np.linalg.norm(q) + 1e-8
        for i, emb in enumerate(self.embeddings):
            denom = q_norm * (np.linalg.norm(emb) + 1e-8)
            if denom == 0.0:
                sim = 0.0
            else:
                sim = float(np.dot(q, emb) / denom)
            sims.append((i, sim))
        sims.sort(key=lambda x: x[1], reverse=True)
        return [(self.documents[idx], score) for idx, score in sims[:k]]

############################################################
# RAG + RL 통합 시스템 (핵심!)
############################################################

class RAGWithRLSystem:
    def __init__(self):
        self.db = DatabaseConnector()
        self.vs = VectorStore()
        self.reward_fn = RewardFunction()
        self.response_filter = ResponseFilter.create_default_filters()
        self.ppo_trainer = PPOTrainer(self.reward_fn, self.response_filter)

    def initialize(self, limit: int = 20000):
        print("\n=== RAG + RL 시스템 초기화 ===")
        if not self.db.connect():
            return False
        docs = self.db.get_policy_chunks(limit)
        if not docs:
            print("[DB] 약관 데이터가 없습니다.")
            self.db.disconnect()
            return False
        texts = [str(d["chunk_text"]) for d in docs]
        print(f"[EMBED] 청크 수: {len(texts)}개, dim={LOCAL_EMBED_DIM}")
        embeddings = simple_embed(texts, dim=LOCAL_EMBED_DIM)
        self.vs.load(docs, embeddings)
        self.db.disconnect()
        print("=== 초기화 완료 ===\n")
        return True

    def answer_with_rl(self, question: str, enable_rl: bool = True,
                      enable_filtering: bool = True) -> Dict:
        if self.vs.embeddings is None:
            return {
                "answer": "RAG가 초기화되지 않았습니다.",
                "sources": [],
                "reward": 0.0,
                "passed_filters": False
            }

        dim = self.vs.embeddings.shape[1]
        q_emb = simple_embed([question], dim=dim)
        retrieved = self.vs.search(q_emb, k=5)

        if not retrieved:
            return {
                "answer": "관련 정보 없음",
                "sources": [],
                "reward": 0.0,
                "passed_filters": False
            }

        context = "\n".join(
            f"[{i+1}] {doc['chunk_text'][:250]}"
            for i, (doc, _) in enumerate(retrieved)
        )
        source_ids = [str(doc["chunk_id"]) for doc, _ in retrieved]

        prompt = f"""
다음은 보험약관의 일부입니다. 이를 참고해 질문에 명확하게 답하세요.

[약관]
{context}

[질문]
{question}

[규칙]
- 약관 내용을 요약해서 말할 것
- 질문에 직접적으로 답하기
- 불필요한 말 금지
- 결론 먼저 말하기

[최종 답변]
"""

        answer = call_llm_api(prompt)

        result = {
            "answer": answer,
            "sources": source_ids,
            "reward": 0.0,
            "passed_filters": True,
            "metadata": {}
        }

        if enable_rl:
            eval_result = self.ppo_trainer.evaluate_response(
                question, answer, source_ids
            )
            result["reward"] = eval_result["reward"]
            result["passed_filters"] = eval_result["passed"]
            result["metadata"]["failed_filters"] = eval_result["failed_filters"]
            result["metadata"]["breakdown"] = eval_result["breakdown"]

            self.ppo_trainer.record_experience(
                question, answer, source_ids,
                eval_result["reward"], eval_result["breakdown"]
            )

        if enable_filtering and not result["passed_filters"]:
            result["answer"] += "\n[주의: 응답이 필터링 검사에 통과하지 못했습니다.]"

        return result

    def train_rl_step(self):
        self.ppo_trainer.update_policy()
        return self.ppo_trainer.get_training_summary()

    def get_rl_status(self) -> Dict:
        return self.ppo_trainer.get_training_summary()

############################################################
# Singleton Access
############################################################

_rag_rl_instance: Optional[RAGWithRLSystem] = None

def init_rag_with_rl():
    global _rag_rl_instance
    if _rag_rl_instance is None:
        rag = RAGWithRLSystem()
        rag.initialize()
        _rag_rl_instance = rag
    return True

def answer_with_rag_rl(question: str, enable_rl: bool = True):
    if _rag_rl_instance is None:
        raise RuntimeError("RAG + RL 초기화 필요")
    return _rag_rl_instance.answer_with_rl(question, enable_rl=enable_rl)

def get_rl_training_status() -> Dict:
    if _rag_rl_instance is None:
        return {}
    return _rag_rl_instance.get_rl_status()

# ===============================================
# RAG + RL 통합 보험약관 분석 시스템 (v2.0)
# 요구사항:
#   1) 사전 벡터 임베딩
#   2) 인덱스 기반 하이브리드 검색
#   3) RL 기반 응답 평가 + 보험사 비교
# ===============================================

import os
import gc
import ctypes
import platform
import numpy as np
import json
import re
from typing import Dict, List, Tuple, Optional, Set
from urllib.parse import urlparse
from dataclasses import dataclass, asdict, field
from datetime import datetime
from enum import Enum
import warnings
import psycopg2
from psycopg2.extras import RealDictCursor
from openai import OpenAI

warnings.filterwarnings("ignore")

# ===============================================
# 1. 메모리 최적화
# ===============================================

def aggressive_memory_cleanup():
    gc.collect()
    if platform.system() == "Linux":
        try:
            ctypes.CDLL("libc.so.6").malloc_trim(0)
        except:
            pass

# ===============================================
# 2. LLM API 설정 (Groq)
# ===============================================

client = OpenAI(
    api_key=os.getenv("GROQ_API_KEY"),
    base_url=os.getenv("GROQ_API_URL", "https://api.groq.com/openai/v1")
)
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama3-70b-8192")

def call_llm_api(prompt: str, temperature: float = 0.2) -> str:
    """LLM 호출"""
    try:
        response = client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[LLM ERROR] {e}")
        return "LLM 생성 오류"

# ===============================================
# 3. 로컬 임베딩 (간단 해시 기반)
# ===============================================

LOCAL_EMBED_DIM = int(os.getenv("LOCAL_EMBED_DIM", "512"))
_token_pattern = re.compile(r"[가-힣A-Za-z0-9]+")

def simple_tokenize(text: str) -> List[str]:
    """토크나이제이션"""
    text = "" if text is None else str(text)
    return _token_pattern.findall(text.lower())

def simple_embed(texts: List[str], dim: int = LOCAL_EMBED_DIM) -> np.ndarray:
    """간단한 해시 기반 임베딩"""
    n = len(texts)
    vecs = np.zeros((n, dim), dtype="float32")
    for i, t in enumerate(texts):
        tokens = simple_tokenize(t)
        for tok in tokens:
            h = hash(tok) % dim
            vecs[i, h] += 1.0
    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-8
    vecs = vecs / norms
    return vecs

# ===============================================
# 4. DB 연결 관리
# ===============================================

class DatabaseConnector:
    """PostgreSQL 연결 및 pgvector 검색"""

    def __init__(self):
        raw_url = os.getenv("DATABASE_URL")
        if not raw_url:
            raise RuntimeError("환경변수 DATABASE_URL 필요")

        if raw_url.startswith("postgres://"):
            raw_url = raw_url.replace("postgres://", "postgresql://", 1)

        parsed = urlparse(raw_url)
        self.config = {
            "host": parsed.hostname,
            "port": parsed.port,
            "database": parsed.path.lstrip("/"),
            "user": parsed.username,
            "password": parsed.password,
        }
        self.connection = None
        print(f"[DB] 설정 로드됨: host={self.config['host']}, db={self.config['database']}")

    def connect(self) -> bool:
        """DB 연결"""
        try:
            self.connection = psycopg2.connect(**self.config)
            print("[DB] 연결 성공")
            return True
        except Exception as e:
            print(f"[DB ERROR] {e}")
            return False

    def disconnect(self):
        """DB 연결 종료"""
        if self.connection:
            self.connection.close()
            self.connection = None

    def ensure_indices(self):
        """인덱스 생성 (사전 처리 단계)"""
        indices = [
            # pgvector HNSW 인덱스 (벡터 검색)
            """
            CREATE INDEX IF NOT EXISTS idx_insurance_vector_hnsw
            ON insurance_clauses USING hnsw (embedding vector_cosine_ops);
            """,

            # 메타데이터 복합 인덱스
            """
            CREATE INDEX IF NOT EXISTS idx_product_id
            ON insurance_clauses (product_id);
            """,

            # 텍스트 검색 인덱스 (GIN)
            """
            CREATE INDEX IF NOT EXISTS idx_search_text_gin
            ON insurance_clauses USING GIN
            (to_tsvector('korean', COALESCE(search_text, original_text)));
            """,
        ]

        try:
            cur = self.connection.cursor()
            for idx_sql in indices:
                cur.execute(idx_sql)
            self.connection.commit()
            cur.close()
            print("[DB] 인덱스 확인 완료")
        except Exception as e:
            print(f"[DB INDEX ERROR] {e}")

    def get_all_chunks(self, limit: int = 50000) -> List[Dict]:
        """모든 청크 조회 (사전 임베딩용)"""
        try:
            cur = self.connection.cursor(cursor_factory=RealDictCursor)
            cur.execute("""
                SELECT
                    id AS chunk_id,
                    product_id,
                    insurance_company,
                    clause_type,
                    COALESCE(search_text, original_text) AS chunk_text,
                    embedding
                FROM insurance_clauses
                WHERE COALESCE(search_text, original_text) IS NOT NULL
                LIMIT %s
            """, (limit,))
            rows = cur.fetchall()
            cur.close()
            print(f"[DB] {len(rows)}개 청크 로드")
            return rows
        except Exception as e:
            print(f"[DB FETCH ERROR] {e}")
            return []

    def update_embeddings(self, chunk_id: int, embedding_bytes: bytes):
        """벡터 업데이트 (pgvector 저장)"""
        try:
            cur = self.connection.cursor()
            cur.execute(
                "UPDATE insurance_clauses SET embedding = %s WHERE id = %s",
                (embedding_bytes, chunk_id)
            )
            self.connection.commit()
            cur.close()
        except Exception as e:
            print(f"[DB UPDATE ERROR] {e}")

    def hybrid_search(self, query_embedding: np.ndarray,
                     filters: Dict[str, any] = None,
                     k: int = 5) -> List[Dict]:
        """
        하이브리드 검색 (메타데이터 필터 + 벡터 유사도)
        """
        default_filters = {
            "product_ids": [],
            "insurance_companies": [],
            "clause_types": [],
            "keywords": [],
            "min_length": 50
        }
        filters = {**default_filters, **(filters or {})}

        # 임베딩을 pgvector 형식으로 변환
        emb_str = "[" + ",".join(str(x) for x in query_embedding[0]) + "]"

        # 동적 WHERE 절 구성
        where_clauses = ["COALESCE(search_text, original_text) IS NOT NULL"]
        params = []

        if filters["product_ids"]:
            where_clauses.append("product_id = ANY(%s::text[])")
            params.append(filters["product_ids"])

        if filters["insurance_companies"]:
            where_clauses.append("insurance_company = ANY(%s::text[])")
            params.append(filters["insurance_companies"])

        if filters["clause_types"]:
            where_clauses.append("clause_type = ANY(%s::text[])")
            params.append(filters["clause_types"])

        if filters["min_length"]:
            where_clauses.append("LENGTH(COALESCE(search_text, original_text)) >= %s")
            params.append(filters["min_length"])

        where_sql = " AND ".join(where_clauses)

        query = f"""
        SELECT
            id AS chunk_id,
            product_id,
            insurance_company,
            clause_type,
            COALESCE(search_text, original_text) AS chunk_text,
            embedding <=> '{emb_str}'::vector AS distance
        FROM insurance_clauses
        WHERE {where_sql}
        ORDER BY embedding <=> '{emb_str}'::vector
        LIMIT %s
        """

        params.append(k)

        try:
            cur = self.connection.cursor(cursor_factory=RealDictCursor)
            cur.execute(query, params)
            rows = cur.fetchall()
            cur.close()
            print(f"[HYBRID] {len(rows)}개 검색 (필터: {filters})")
            return rows
        except Exception as e:
            print(f"[HYBRID ERROR] {e}")
            return []

# ===============================================
# 5. 강화학습: Reward 함수 (개선)
# ===============================================

@dataclass
class RewardConfig:
    """보험약관 평가용 보상 설정"""
    structure_weight: float = 0.25      # 구조적 명확성
    information_weight: float = 0.30    # 구체적 정보 (숫자, 금액)
    clarity_weight: float = 0.20        # 명료성 (불확실성 없음)
    relevance_weight: float = 0.15      # 질문과의 연관성
    accuracy_weight: float = 0.10       # 정확성 (조항 번호 등)

class RewardFunction:
    """보험약관 답변 평가 (품질 스코어링)"""

    def __init__(self, config: RewardConfig = None):
        self.config = config or RewardConfig()

    def __call__(self, question: str, answer: str,
                sources: List[str] = None) -> Tuple[float, Dict]:
        """총 보상 점수 계산"""
        sources = sources or []
        breakdown = {}
        total_reward = 0.0

        # 1. 구조 평가
        structure_score = self._eval_structure(answer)
        breakdown['structure'] = structure_score
        total_reward += structure_score * self.config.structure_weight

        # 2. 정보성 평가
        information_score = self._eval_information(answer, question)
        breakdown['information'] = information_score
        total_reward += information_score * self.config.information_weight

        # 3. 명료성 평가
        clarity_score = self._eval_clarity(answer)
        breakdown['clarity'] = clarity_score
        total_reward += clarity_score * self.config.clarity_weight

        # 4. 적합성 평가
        relevance_score = self._eval_relevance(question, answer)
        breakdown['relevance'] = relevance_score
        total_reward += relevance_score * self.config.relevance_weight

        # 5. 정확성 평가 (조항 인용 여부)
        accuracy_score = self._eval_accuracy(answer, sources)
        breakdown['accuracy'] = accuracy_score
        total_reward += accuracy_score * self.config.accuracy_weight

        final_reward = min(total_reward, 1.0)
        breakdown['total'] = final_reward

        return final_reward, breakdown

    def _eval_structure(self, answer: str) -> float:
        """구조성 평가"""
        score = 0.0
        if re.search(r"제\d+조", answer):
            score += 0.4
        if re.search(r"[\d]+\)", answer):
            score += 0.3
        if len(answer.split("\n")) >= 2:
            score += 0.3
        return min(score, 1.0)

    def _eval_information(self, answer: str, question: str) -> float:
        """정보성 평가 (구체적 수치)"""
        score = 0.0
        if re.search(r"\d+년|월|일|주", answer):
            score += 0.35
        if re.search(r"\d+원|만원|억원|\d+%", answer):
            score += 0.35
        if 100 < len(answer) < 3000:
            score += 0.3
        return min(score, 1.0)

    def _eval_clarity(self, answer: str) -> float:
        """명료성 평가 (불확실한 표현 감점)"""
        score = 1.0
        uncertain_phrases = [
            ("아마도", 0.15), ("것 같다", 0.15), ("추정된다", 0.1),
            ("가능할 것", 0.15), ("일 수 있다", 0.1), ("대체로", 0.05),
        ]
        for phrase, penalty in uncertain_phrases:
            if phrase in answer:
                score -= penalty
        return max(score, 0.0)

    def _eval_relevance(self, question: str, answer: str) -> float:
        """적합성 평가 (질문-답변 토큰 중복)"""
        q_words = set(simple_tokenize(question))
        a_words = set(simple_tokenize(answer))
        if not q_words:
            return 0.5
        overlap = len(q_words & a_words) / len(q_words)
        return min(overlap, 1.0)

    def _eval_accuracy(self, answer: str, sources: List[str]) -> float:
        """정확성 평가 (출처/조항 인용)"""
        score = 0.0
        if len(sources) >= 2:
            score += 0.4
        if re.search(r"제\d+조|조항", answer):
            score += 0.3
        if re.search(r"\[출처\]|\[조항\]|보험약관", answer):
            score += 0.3
        return min(score, 1.0)

# ===============================================
# 6. 강화학습: Response Filter
# ===============================================

class ResponseFilter:
    """응답 품질 필터링 (최소 기준 강제)"""

    def __init__(self):
        self.filters: Dict[str, callable] = {}

    def add_filter(self, name: str, condition: callable):
        """필터 추가"""
        self.filters[name] = condition

    def apply_filters(self, answer: str, context: Dict) -> Tuple[bool, List[str]]:
        """모든 필터 적용"""
        failed = []
        for name, condition in self.filters.items():
            try:
                if not condition(answer, context):
                    failed.append(name)
            except Exception as e:
                print(f"[FILTER ERROR] {name}: {e}")
                failed.append(name)
        return len(failed) == 0, failed

    @staticmethod
    def create_default_filters() -> "ResponseFilter":
        """기본 필터 생성"""
        f = ResponseFilter()

        # 1. 길이 필터
        f.add_filter("length", lambda a, c: 50 < len(a) < 5000)

        # 2. 한글 포함 필터
        f.add_filter("korean", lambda a, c: any("\uac00" <= ch <= "\ud7a3" for ch in a))

        # 3. 금지어 필터
        def no_forbidden(answer, context):
            forbidden = ["모릅니다", "알 수 없습니다", "기재되지 않았습니다", "불명확합니다"]
            return not any(fw in answer for fw in forbidden)
        f.add_filter("no_forbidden", no_forbidden)

        # 4. 신뢰도 필터 (reward >= 0.4)
        f.add_filter("confidence", lambda a, c: c.get("reward", 0) >= 0.4)

        # 5. 출처 필터 (최소 1개 이상)
        f.add_filter("sources", lambda a, c: len(c.get("sources", [])) >= 1)

        # 6. 약관 내용 포함 필터
        f.add_filter("insurance_content",
                    lambda a, c: any(keyword in a for keyword in ["조항", "원", "기간", "보험"]))

        return f

# ===============================================
# 7. 강화학습: Experience & PPO Trainer
# ===============================================

@dataclass
class RLExperience:
    """RL 경험 데이터"""
    question: str
    answer: str
    reward: float
    breakdown: Dict
    timestamp: str
    sources: List[str]
    insurance_companies: List[str] = field(default_factory=list)
    passed_filters: bool = True

class PPOTrainer:
    """Policy Gradient 기반 학습"""

    def __init__(self, reward_fn: RewardFunction,
                response_filter: ResponseFilter = None):
        self.reward_fn = reward_fn
        self.response_filter = response_filter or ResponseFilter.create_default_filters()
        self.experiences: List[RLExperience] = []
        self.training_stats = {
            "avg_reward": 0.0,
            "max_reward": 0.0,
            "min_reward": 1.0,
            "total_samples": 0,
            "pass_rate": 0.0,
            "avg_sources": 0.0,
        }
        self.policy_version = 1

    def evaluate_response(self, question: str, answer: str,
                         sources: List[str] = None,
                         insurance_companies: List[str] = None) -> Dict:
        """응답 평가"""
        sources = sources or []
        insurance_companies = insurance_companies or []

        # 1. Reward 계산
        reward, breakdown = self.reward_fn(question, answer, sources)

        # 2. 필터링 적용
        context = {
            "reward": reward,
            "sources": sources,
            "breakdown": breakdown
        }
        passed, failed_filters = self.response_filter.apply_filters(answer, context)

        # 3. 최종 보상 계산 (필터 통과 못하면 감점)
        final_reward = reward if passed else max(reward - 0.3, 0.0)

        return {
            "reward": final_reward,
            "breakdown": breakdown,
            "passed": passed,
            "failed_filters": failed_filters,
            "sources": sources,
            "insurance_companies": insurance_companies
        }

    def record_experience(self, question: str, answer: str,
                         sources: List[str], reward: float, breakdown: Dict,
                         insurance_companies: List[str] = None, passed: bool = True):
        """경험 기록"""
        exp = RLExperience(
            question=question,
            answer=answer,
            reward=reward,
            breakdown=breakdown,
            timestamp=datetime.now().isoformat(),
            sources=sources,
            insurance_companies=insurance_companies or [],
            passed_filters=passed
        )
        self.experiences.append(exp)

    def update_policy(self, learning_rate: float = 0.01):
        """정책 업데이트 (통계 계산)"""
        if len(self.experiences) < 2:
            return

        rewards = np.array([e.reward for e in self.experiences])
        sources_count = np.array([len(e.sources) for e in self.experiences])

        self.training_stats["avg_reward"] = float(np.mean(rewards))
        self.training_stats["max_reward"] = float(np.max(rewards))
        self.training_stats["min_reward"] = float(np.min(rewards))
        self.training_stats["total_samples"] = len(self.experiences)
        self.training_stats["avg_sources"] = float(np.mean(sources_count))

        pass_count = sum(1 for e in self.experiences if e.reward >= 0.5)
        self.training_stats["pass_rate"] = pass_count / len(self.experiences)

        self.policy_version += 1
        print(f"[POLICY] 버전 {self.policy_version}, 통과율: {self.training_stats['pass_rate']:.2%}")

    def get_training_summary(self) -> Dict:
        """학습 요약"""
        return {
            "policy_version": self.policy_version,
            "statistics": self.training_stats,
            "total_experiences": len(self.experiences),
            "recent_experiences": [asdict(e) for e in self.experiences[-5:]]
        }

# ===============================================
# 8. 보험사 비교 분석기
# ===============================================

class InsuranceComparator:
    """보험사별 약관 비교 분석"""

    def __init__(self, db: DatabaseConnector):
        self.db = db

    def compare_clauses(self, question: str, clause_type: str = None) -> Dict:
        """
        보험사별 동일 조항 비교
        예: 면책사항 질문 -> 모든 보험사의 면책 조항 비교
        """
        # 질문 기반 임베딩
        q_emb = simple_embed([question], dim=LOCAL_EMBED_DIM)

        # 필터: 모든 보험사, 특정 조항 유형
        filters = {
            "clause_types": [clause_type] if clause_type else ["exclusion", "coverage", "period"],
            "min_length": 50
        }

        # 하이브리드 검색
        results = self.db.hybrid_search(q_emb, filters, k=20)

        # 보험사별 그룹핑
        by_company: Dict[str, List[Dict]] = {}
        for doc in results:
            company = doc.get("insurance_company", "Unknown")
            if company not in by_company:
                by_company[company] = []
            by_company[company].append(doc)

        # 비교 분석
        comparison = {
            "question": question,
            "clause_type": clause_type,
            "companies_compared": list(by_company.keys()),
            "comparison_data": {},
            "summary": ""
        }

        for company, docs in by_company.items():
            comparison["comparison_data"][company] = {
                "clause_count": len(docs),
                "clauses": [
                    {
                        "text": doc["chunk_text"][:300],
                        "type": doc.get("clause_type", "unknown"),
                        "source_id": doc["chunk_id"]
                    }
                    for doc in docs[:3]  # 상위 3개만
                ]
            }

        # LLM 기반 비교 요약
        comparison_prompt = self._build_comparison_prompt(comparison)
        summary = call_llm_api(comparison_prompt, temperature=0.3)
        comparison["summary"] = summary

        return comparison

    def _build_comparison_prompt(self, comparison: Dict) -> str:
        """비교 프롬프트 생성"""
        companies_text = "\n".join([
            f"\n[{company}]\n" + "\n".join([
                f"  - {clause['text'][:100]}..."
                for clause in comparison["comparison_data"][company]["clauses"]
            ])
            for company in comparison["comparison_data"].keys()
        ])

        prompt = f"""
다음은 여러 보험사의 약관 비교 데이터입니다.

[질문]
{comparison['question']}

[약관 유형]
{comparison['clause_type'] or '전체'}

[보험사별 약관 조항]
{companies_text}

[요구사항]
1) 보험사별 차이점 분석
2) 소비자에게 유리한 조항 강조
3) 주의사항 지적
4) 추천 사항 제시

[형식]
- 각 보험사별 평가
- 종합 비교 분석
- 선택 가이드
        """
        return prompt

    def get_accuracy_metrics(self, experiences: List[RLExperience]) -> Dict:
        """
        요구사항 i) 평가: 약관 내에서 정확한 정보 발견률
        """
        total = len(experiences)
        if total == 0:
            return {"accuracy": 0.0, "details": {}}

        accuracy_scores = []
        for exp in experiences:
            # breakdown에서 accuracy 스코어 추출
            accuracy_scores.append(exp.breakdown.get("accuracy", 0.0))

        avg_accuracy = np.mean(accuracy_scores)
        high_accuracy = sum(1 for s in accuracy_scores if s >= 0.7)

        return {
            "overall_accuracy": float(avg_accuracy),
            "high_accuracy_count": high_accuracy,
            "high_accuracy_rate": high_accuracy / total,
            "details": {
                "tested_questions": total,
                "avg_sources_per_answer": np.mean([len(e.sources) for e in experiences]),
                "avg_reward": np.mean([e.reward for e in experiences])
            }
        }

# ===============================================
# 9. 통합 RAG+RL 시스템
# ===============================================

class RAGWithRLSystem:
    """보험약관 RAG+RL 통합 시스템"""

    def __init__(self):
        self.db = DatabaseConnector()
        self.reward_fn = RewardFunction()
        self.response_filter = ResponseFilter.create_default_filters()
        self.ppo_trainer = PPOTrainer(self.reward_fn, self.response_filter)
        self.comparator = InsuranceComparator(self.db)
        self.embeddings_cache: Dict[int, np.ndarray] = {}

    def initialize(self, limit: int = 50000):
        """
        [요구사항 1] 사전 임베딩 처리
        - DB의 모든 청크를 임베딩하여 pgvector에 저장
        """
        print("\n=== RAG+RL 시스템 초기화 시작 ===")

        # DB 연결
        if not self.db.connect():
            return False

        # 인덱스 생성
        self.db.ensure_indices()

        # 1. 모든 청크 로드
        chunks = self.db.get_all_chunks(limit)
        if not chunks:
            print("[INIT] 청크 데이터 없음")
            self.db.disconnect()
            return False

        # 2. 벡터 임베딩 생성
        print(f"[EMBED] {len(chunks)}개 청크 임베딩 중...")
        texts = [chunk["chunk_text"] for chunk in chunks]
        embeddings = simple_embed(texts, dim=LOCAL_EMBED_DIM)

        # 3. pgvector에 저장
        print("[UPDATE] pgvector DB 업데이트 중...")
        for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):
            if i % 1000 == 0:
                print(f"  {i}/{len(chunks)}")

            # 임베딩을 pgvector 문자열 형식으로 변환
            emb_str = "[" + ",".join(str(x) for x in emb) + "]"

            # DB 업데이트
            try:
                cur = self.db.connection.cursor()
                cur.execute(
                    "UPDATE insurance_clauses SET embedding = %s::vector WHERE id = %s",
                    (emb_str, chunk["chunk_id"])
                )
                self.db.connection.commit()
                cur.close()
            except Exception as e:
                print(f"[UPDATE ERROR] {e}")

            # 캐시 저장 (빠른 검색용)
            self.embeddings_cache[chunk["chunk_id"]] = emb

        self.db.disconnect()
        print("=== 초기화 완료 ===\n")
        return True

    def answer_with_rl(self, question: str, enable_rl: bool = True,
                      enable_filtering: bool = True,
                      search_filters: Dict[str, any] = None,
                      enable_comparison: bool = False) -> Dict:
        """
        [요구사항 2 & 3] 하이브리드 검색 + LLM + RL 평가
        """
        # DB 재연결
        if not self.db.connect():
            return {
                "answer": "DB 연결 실패",
                "sources": [],
                "reward": 0.0,
                "passed_filters": False
            }

        # 1. 질문 임베딩
        q_emb = simple_embed([question], dim=LOCAL_EMBED_DIM)

        # 2. 하이브리드 검색 (인덱스 기반)
        search_filters = search_filters or {}
        retrieved = self.db.hybrid_search(q_emb, search_filters, k=5)

        self.db.disconnect()

        if not retrieved:
            return {
                "answer": "관련 약관 정보를 찾을 수 없습니다.",
                "sources": [],
                "reward": 0.0,
                "passed_filters": False
            }

        # 3. 컨텍스트 구성
        context = "\n".join(
            f"[{i+1}] {doc['chunk_text'][:300]}"
            for i, doc in enumerate(retrieved)
        )
        source_ids = [str(doc["chunk_id"]) for doc in retrieved]
        insurance_companies = list(set(doc.get("insurance_company", "Unknown") for doc in retrieved))

        # 4. LLM 프롬프트 생성
        prompt = f"""
당신은 보험약관 전문가입니다. 다음 약관 내용을 바탕으로 질문에 정확하고 명확하게 답하세요.

[약관 내용]
{context}

[질문]
{question}

[답변 규칙]
- 약관에 명시된 내용만 사용
- 구체적인 조항 번호 인용 (예: 제5조)
- 금액, 기간 등 구체적 정보 포함
- 불확실한 표현 금지 (반드시 "~합니다", "~입니다" 형태 사용)
- 출처 명시

[최종 답변]
"""

        # 5. LLM 호출
        answer = call_llm_api(prompt)

        # 6. RL 평가
        result = {
            "question": question,
            "answer": answer,
            "sources": source_ids,
            "insurance_companies": insurance_companies,
            "retrieved_documents": len(retrieved),
            "reward": 0.0,
            "passed_filters": True,
            "metadata": {}
        }

        if enable_rl:
            eval_result = self.ppo_trainer.evaluate_response(
                question, answer, source_ids, insurance_companies
            )
            result["reward"] = eval_result["reward"]
            result["passed_filters"] = eval_result["passed"]
            result["metadata"]["failed_filters"] = eval_result["failed_filters"]
            result["metadata"]["breakdown"] = eval_result["breakdown"]

            # 경험 기록
            self.ppo_trainer.record_experience(
                question, answer, source_ids,
                eval_result["reward"], eval_result["breakdown"],
                insurance_companies, eval_result["passed"]
            )

        if enable_filtering and not result["passed_filters"]:
            result["answer"] += "\n⚠️ [주의] 이 답변은 품질 검사에 통과하지 못했습니다."

        # 7. 보험사 비교 (선택)
        if enable_comparison:
            comparison = self.comparator.compare_clauses(question)
            result["comparison"] = comparison

        return result

    def train_rl_step(self):
        """정책 업데이트"""
        self.ppo_trainer.update_policy()
        return self.ppo_trainer.get_training_summary()

    def get_rl_status(self) -> Dict:
        """RL 학습 상태"""
        return self.ppo_trainer.get_training_summary()

    def get_accuracy_report(self) -> Dict:
        """
        [요구사항 i) 정확성 보고서]
        """
        return self.comparator.get_accuracy_metrics(self.ppo_trainer.experiences)

    def get_comparison_report(self) -> Dict:
        """
        [요구사항 ii) 보험사 비교 보고서]
        """
        if not self.ppo_trainer.experiences:
            return {"error": "비교 데이터 없음"}

        companies: Set[str] = set()
        for exp in self.ppo_trainer.experiences:
            companies.update(exp.insurance_companies)

        return {
            "companies_analyzed": list(companies),
            "total_questions_answered": len(self.ppo_trainer.experiences),
            "avg_reward_per_company": self._calc_avg_reward_by_company(),
            "policy_version": self.ppo_trainer.policy_version
        }

    def _calc_avg_reward_by_company(self) -> Dict[str, float]:
        """보험사별 평균 보상"""
        rewards_by_company: Dict[str, List[float]] = {}
        for exp in self.ppo_trainer.experiences:
            for company in exp.insurance_companies:
                if company not in rewards_by_company:
                    rewards_by_company[company] = []
                rewards_by_company[company].append(exp.reward)

        return {
            company: float(np.mean(rewards))
            for company, rewards in rewards_by_company.items()
        }

# ===============================================
# 10. Singleton 패턴 (글로벌 인스턴스)
# ===============================================

_rag_rl_instance: Optional[RAGWithRLSystem] = None

def init_rag_with_rl(limit: int = 50000) -> bool:
    """시스템 초기화"""
    global _rag_rl_instance
    if _rag_rl_instance is None:
        print("[INIT] RAG+RL 시스템 생성 중...")
        rag = RAGWithRLSystem()
        if rag.initialize(limit):
            _rag_rl_instance = rag
            return True
        return False
    return True

def answer_question(question: str, enable_rl: bool = True,
                   enable_comparison: bool = False,
                   search_filters: Dict[str, any] = None) -> Dict:
    """질문 답변"""
    if _rag_rl_instance is None:
        raise RuntimeError("RAG+RL 시스템 초기화 필요: init_rag_with_rl() 호출")
    return _rag_rl_instance.answer_with_rl(
        question, enable_rl=enable_rl, enable_filtering=True,
        search_filters=search_filters, enable_comparison=enable_comparison
    )

def train_rl() -> Dict:
    """RL 정책 업데이트"""
    if _rag_rl_instance is None:
        return {}
    return _rag_rl_instance.train_rl_step()

def get_accuracy_report() -> Dict:
    """요구사항 i) 정확성 평가"""
    if _rag_rl_instance is None:
        return {}
    return _rag_rl_instance.get_accuracy_report()

def get_comparison_report() -> Dict:
    """요구사항 ii) 보험사 비교 평가"""
    if _rag_rl_instance is None:
        return {}
    return _rag_rl_instance.get_comparison_report()

def get_system_status() -> Dict:
    """시스템 상태"""
    if _rag_rl_instance is None:
        return {"status": "not_initialized"}
    return {
        "status": "ready",
        "rl_summary": _rag_rl_instance.get_rl_status(),
        "accuracy_report": _rag_rl_instance.get_accuracy_report(),
        "comparison_report": _rag_rl_instance.get_comparison_report()
    }

# ===============================================
# 11. 메인 실행 예시
# ===============================================

if __name__ == "__main__":

    # 1. 시스템 초기화 (사전 벡터 임베딩)
    print("▶ 시스템 초기화 (사전 벡터 임베딩)...")
    if not init_rag_with_rl(limit=10000):
        print("❌ 초기화 실패")
        exit(1)

    print("\n✅ 초기화 완료\n")

    # 2. 질문 처리
    questions = [
        {
            "text": "P001 보험의 면책사항은 무엇인가?",
            "filters": {"product_ids": ["P001"]}
        },
        {
            "text": "보장 기간은 얼마나 되나?",
            "filters": {}
        },
        {
            "text": "청구 절차는 어떻게 되나?",
            "filters": {"clause_types": ["procedure"]}
        }
    ]

    print("▶ 질문 처리 시작...\n")
    for i, q in enumerate(questions, 1):
        print(f"질문 {i}: {q['text']}")
        result = answer_question(q["text"], search_filters=q["filters"], enable_comparison=True)

        print(f"답변: {result['answer'][:200]}...")
        print(f"출처: {result['sources'][:2]}")
        print(f"보상: {result['reward']:.2f}")
        print(f"통과: {result['passed_filters']}")
        print()

    # 3. RL 정책 업데이트
    print("▶ RL 정책 업데이트...")
    train_result = train_rl()
    print(f"정책 버전: {train_result['policy_version']}")
    print(f"통과율: {train_result['statistics']['pass_rate']:.2%}")
    print()

    # 4. 요구사항 평가 보고서
    print("=" * 50)
    print("요구사항 평가 보고서")
    print("=" * 50)

    print("\n[i) 정확성 평가]")
    accuracy = get_accuracy_report()
    print(f"전체 정확성: {accuracy['overall_accuracy']:.2%}")
    print(f"고품질 답변율: {accuracy['high_accuracy_rate']:.2%}")
    print(f"평균 소스 개수: {accuracy['details']['avg_sources_per_answer']:.1f}")

    print("\n[ii) 보험사 비교]")
    comparison = get_comparison_report()
    print(f"비교 대상 보험사: {comparison['companies_analyzed']}")
    print(f"총 답변 개수: {comparison['total_questions_answered']}")
    print(f"보험사별 평균 보상:")
    for company, reward in comparison['avg_reward_per_company'].items():
        print(f"  - {company}: {reward:.2f}")

# rl_유사도_필터링.py 맨 아래 쪽에 추가

def init_rag(limit: int = 20000):
    # 기존 FastAPI가 호출하는 초기화 래퍼
    return init_rag_with_rl(limit=limit)

def answer_with_rag(question: str):
    # 기존 FastAPI가 호출하는 답변 래퍼
    return answer_question(question, enable_rl=True, enable_comparison=False)
