# -*- coding: utf-8 -*-
"""시연용 모델의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qtOWY_naUOww8I91MFsBjWD-kJ3ZkVug
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
메모리 효율적 보험약관 RAG 챗봇 - 순차적 모델 로딩
"""
# Colab 셀에 붙여넣기
!pip install -q transformers FlagEmbedding psycopg2-binary torch numpy

# 위 코드 복사-붙여넣기 후 실행

import gc
import torch
import ctypes
import platform
from typing import List, Dict, Tuple, Optional
import numpy as np
from urllib.parse import urlparse
import warnings
warnings.filterwarnings('ignore')

DATABASE_URL = "postgresql://postgres:cclGH8x.3nT.ru1mf92xjJaZwP3xY5ev@hopper.proxy.rlwy.net:13755/DB_Insurance"

parsed_url = urlparse(DATABASE_URL)
db_config = {
    'host': parsed_url.hostname,
    'port': parsed_url.port,
    'database': parsed_url.path[1:],
    'user': parsed_url.username,
    'password': parsed_url.password
}

# ============================================================
# 메모리 정리 함수
# ============================================================

def aggressive_memory_cleanup():
    """공격적인 메모리 정리"""
    gc.collect()

    # Linux/macOS에서 메모리 완전 해제
    if platform.system() == "Linux":
        try:
            ctypes.CDLL("libc.so.6").malloc_trim(0)
        except:
            pass
    elif platform.system() == "Darwin":
        try:
            libc = ctypes.CDLL("libc.dylib")
            libc.malloc_zone_pressure_relief(0, 0)
        except:
            pass

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    print("✓ 메모리 정리 완료")

# ============================================================
# DATABASE CONNECTOR
# ============================================================

class DatabaseConnector:
    def __init__(self, config: Dict):
        self.config = config
        self.connection = None

    def connect(self) -> bool:
        try:
            import psycopg2
            self.connection = psycopg2.connect(**self.config)
            print("✓ DB 연결 성공")
            return True
        except Exception as e:
            print(f"✗ DB 연결 오류: {e}")
            return False

    def disconnect(self):
        if self.connection:
            self.connection.close()

    def get_all_documents(self, table_name: str = "policychunk", limit: int = 50):
        """문서 조회 (30개 제한)"""
        try:
            import psycopg2
            from psycopg2.extras import RealDictCursor

            cursor = self.connection.cursor(cursor_factory=RealDictCursor)
            query = f"SELECT * FROM {table_name} LIMIT %s"
            cursor.execute(query, (limit,))
            documents = cursor.fetchall()
            cursor.close()
            print(f"✓ {len(documents)}개 문서 조회")
            return documents
        except Exception as e:
            print(f"✗ 문서 조회 오류: {e}")
            return []

# ============================================================
# EMBEDDING MODEL (사용 후 언로드)
# ============================================================

class EmbeddingModel:
    def __init__(self, model_name: str = "BAAI/bge-m3"):
        self.model_name = model_name
        self.model = None
        print(f"임베딩 모델: {model_name}")

    def load(self):
        """모델 로드"""
        print("임베딩 모델 로딩 중...")
        from FlagEmbedding import BGEM3FlagModel

        self.model = BGEM3FlagModel(
            self.model_name,
            use_fp16=False  # CPU에서는 FP32 사용
        )
        print("✓ 임베딩 모델 로드 완료")

    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """텍스트 임베딩"""
        if self.model is None:
            self.load()

        embeddings = self.model.encode(
            texts,
            batch_size=8,  # 작은 배치
            return_dense=True,
            return_sparse=False,
            return_colbert_vecs=False
        )
        return embeddings['dense_vecs']

    def unload(self):
        """모델 언로드"""
        print("임베딩 모델 언로드 중...")
        del self.model
        self.model = None
        aggressive_memory_cleanup()
        print("✓ 임베딩 모델 언로드 완료")

# ============================================================
# VECTOR STORE (임베딩만 저장)
# ============================================================

class VectorStore:
    def __init__(self):
        self.documents = []
        self.embeddings = np.array([])

    def add_documents_with_embeddings(self, documents: List[Dict], embeddings: np.ndarray):
        """이미 계산된 임베딩과 문서 추가"""
        self.documents = documents
        self.embeddings = embeddings
        print(f"✓ {len(documents)}개 문서 저장")

    def search(self, query_embedding: np.ndarray, k: int = 3) -> List[Tuple[Dict, float]]:
        """유사 문서 검색"""
        if len(self.embeddings) == 0:
            return []

        if query_embedding.ndim > 1:
            query_embedding = query_embedding[0]

        similarities = []
        for i, emb in enumerate(self.embeddings):
            if emb.ndim > 1:
                emb = emb[0]

            sim = np.dot(query_embedding, emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(emb))
            similarities.append((i, float(sim)))

        similarities.sort(key=lambda x: x[1], reverse=True)

        results = []
        for idx, score in similarities[:k]:
            results.append((self.documents[idx], score))

        return results

# ============================================================
# LANGUAGE MODEL (필요할 때만 로드)
# ============================================================

class LanguageModel:
    def __init__(self, model_name: str = "Qwen/Qwen2.5-1.5B-Instruct"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        print(f"언어 모델: {model_name}")

    def load(self):
        """모델 로드"""
        print("언어 모델 로딩 중...")
        from transformers import AutoTokenizer, AutoModelForCausalLM

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )

        self.model.eval()
        print("✓ 언어 모델 로드 완료")

    def generate_response(self, prompt: str, max_new_tokens: int = 1000) -> str:
        """응답 생성"""
        if self.model is None:
            self.load()

        try:
            with torch.no_grad():
                inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1000)

                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                response = response.replace(prompt, "").strip()

            del inputs, outputs
            aggressive_memory_cleanup()

            return response
        except Exception as e:
            print(f"✗ 생성 오류: {e}")
            return "답변을 생성할 수 없습니다."

    def unload(self):
        """모델 언로드"""
        print("언어 모델 언로드 중...")
        del self.model
        del self.tokenizer
        self.model = None
        self.tokenizer = None
        aggressive_memory_cleanup()
        print("✓ 언어 모델 언로드 완료")

# ============================================================
# RAG SYSTEM (순차적 로딩)
# ============================================================

class RAGSystem:
    def __init__(self, db_config: Dict):
        print("\n" + "="*60)
        print("순차적 로딩 RAG 시스템 초기화")
        print("="*60)

        self.embedding_model = EmbeddingModel("BAAI/bge-m3")
        self.llm = LanguageModel("Qwen/Qwen2.5-1.5B-Instruct")
        self.vector_store = VectorStore()
        self.db_connector = DatabaseConnector(db_config)

        print("✓ RAG 시스템 초기화 완료")

    def initialize_knowledge_base(self, limit: int = 30):
        """지식 베이스 초기화 (순차적)"""
        print("\n[1단계] DB 연결 및 문서 조회")
        if not self.db_connector.connect():
            return False

        documents = self.db_connector.get_all_documents(limit=limit)
        if not documents:
            print("✗ 문서 없음")
            return False

        # 텍스트 추출
        texts = []
        for doc in documents:
            text = doc.get('chunk_text')
            #text = doc.get('clause_text') or doc.get('content') or str(doc)
            texts.append(str(text)[:1000])  # 500자 제한

        print(f"\n[2단계] 임베딩 생성 ({len(texts)}개)")
        self.embedding_model.load()
        embeddings = self.embedding_model.encode_texts(texts)

        # ✅ 임베딩 완료 후 즉시 모델 언로드
        self.embedding_model.unload()

        print(f"\n[3단계] 벡터 저장소에 저장")
        self.vector_store.add_documents_with_embeddings(documents, embeddings)

        self.db_connector.disconnect()

        print("✓ 지식 베이스 초기화 완료\n")
        return True

    def answer_question(self, question: str, k: int = 5) -> Dict:
        """질문 답변"""
        print(f"\n{'='*60}")
        print(f"질문: {question}")
        print(f"{'='*60}")

        # 질문 임베딩 (임베딩 모델 임시 로드)
        print("\n[1단계] 질문 임베딩")
        self.embedding_model.load()
        query_emb = self.embedding_model.encode_texts([question])
        self.embedding_model.unload()  # 즉시 언로드

        # 문서 검색
        print("\n[2단계] 유사 문서 검색")
        retrieved_docs = self.vector_store.search(query_emb, k=k)

        if not retrieved_docs:
            return {'question': question, 'answer': '관련 정보 없음'}

        for i, (doc, score) in enumerate(retrieved_docs):
            print(f"  - 문서 {i+1} (유사도: {score:.4f})")

        # 컨텍스트 생성
        context_parts = []
        for i, (doc, score) in enumerate(retrieved_docs):
            text = doc.get('chunk_text')
            #text = doc.get('clause_text') or doc.get('content') or str(doc)
            context_parts.append(f"[{i+1}] {text[:250]}")

        context = "\n".join(context_parts)

        # 프롬프트 생성
        prompt = f"""보험약관 정보:
{context}

질문: {question}

답변:"""

        # 답변 생성 (생성 모델 로드)
        print("\n[3단계] 답변 생성")
        self.llm.load()
        answer = self.llm.generate_response(prompt, max_new_tokens=350)
        self.llm.unload()  # 즉시 언로드

        print("\n✓ 답변 완료")

        return {
            'question': question,
            'answer': answer
        }

    def cleanup(self):
        """리소스 정리"""
        self.embedding_model.unload()
        self.llm.unload()
        aggressive_memory_cleanup()

# ============================================================
# DEMO
# ============================================================

def run_demo():
    print(f"\n{'='*60}")
    print("순차적 로딩 RAG 챗봇 데모")
    print(f"{'='*60}")

    rag_system = RAGSystem(db_config)

    if not rag_system.initialize_knowledge_base(limit=50):
        return

    print("챗봇에 질문을 입력하세요. 종료하려면 '종료' 또는 'exit' 입력")
    while True:
        user_input = input("질문 입력: ").strip()
        if user_input.lower() in ['종료', 'exit']:
            print("챗봇 종료합니다.")
            break
        if not user_input:
            print("질문을 입력해주세요.")
            continue

        result = rag_system.answer_question(user_input, k=5)
        print(f"\n답변: {result['answer']}\n")
        print("─"*60)

    rag_system.cleanup()
    print("\n✓ 데모 완료")

if __name__ == "__main__":
    run_demo()

if __name__ == "__main__":
    run_demo()


def run_demo():
    print(f"\n{'='*60}")
    print("순차적 로딩 RAG 챗봇 데모")
    print(f"{'='*60}")

    rag_system = RAGSystem(db_config)

    if not rag_system.initialize_knowledge_base(limit=50):
        return

    demo_questions = [
        "강아지의 선천적인 질병에 대해서도 보장받을 수 있나요?"
    ]

    for question in demo_questions:
        result = rag_system.answer_question(question, k=5)
        print(f"\n답변: {result['answer']}\n")
        print("─"*60)

    rag_system.cleanup()
    print("\n✓ 데모 완료")

### token 수치, context 분량, text 분량 등은 조정 필요